[{"uri":"/deeplens-rekognition-workshop/","title":"AWS DeepLens 로 얼굴 인식 기기 만들기","tags":[],"description":"","content":"AWS DeepLens 로 얼굴 인식 기기 만들기  AWS DeepLens는 딥 러닝 기술을 확장하도록 설계된 완전히 프로그래밍 가능한 비디오카메라, 자습서, 코드 및 사전 교육된 모델을 통해 쉽고 빠르게 기계학습을 구현할 수 있는 도구입니다.\n이 워크샵에서는 AWS DeepLens 와 Amazon Rekognition 을 이용하여 영상에서 사람의 얼굴을 인식하고, 신원을 확인하는 시스템을 구축합니다.\n "},{"uri":"/deeplens-rekognition-workshop/lab0/","title":"워크샵 시작 전 준비 사항","tags":[],"description":"","content":"DeepLens 등록하기 워크샵을 시작하기에 앞서, 작동하는 DeepLens 기기와 AWS 계정을 준비해 주십시오. 워크샵을 진행하기 위해 DeepLens 기기를 계정에 등록해야 합니다.\nMFA 가 활성화되지 않은 계정은 등록을 진행할 수 없습니다. 과정을 시작하기 앞서, IAM 콘솔에서 MFA 를 등록해 주세요.\n AWS DeepLens 콘솔창 에 접속한 후, Register device 버튼을 누릅니다. 가지고 있는 기기의 버전을 확인한 후, Start 버튼을 누릅니다. 화면의 안내를 따라, 기기 연결을 시작합니다.  DeepLens 기기를 전원에 연결합니다. 전원 버튼을 누르면, 노란 불빛이 깜빡이다가, 파란 불빛이 들어옵니다. Registration 이라고 적힌 USB 포트를 이용해 컴퓨터와 기기를 연결합니다. 연결이 완료되었으면, Detect AWS DeepLens device 버튼을 누르고, 화면 하단의 Next 버튼이 활성화 될 때까지 기다립니다. 화면 안내를 따라, 기기 하단에 기재되어 있는 serial number 를 입력합니다. 네트워크 환경을 확인한 후, Next 버튼을 누릅니다. Device Name 을 설정하고, Permission 부분의 체크박스를 체크한 후, Register Device 버튼을 누릅니다. Device 페이지로 이동하면, Device Status 가 In progress 에서 Registered 로 바뀔 때까지 기다립니다. 등록이 완료되면 다음과 같은 화면이 나옵니다.     "},{"uri":"/deeplens-rekognition-workshop/lab1/","title":"얼굴 인식 프로젝트 배포하기","tags":[],"description":"","content":"프로젝트 생성하기 DeepLens 에서 제공하는 모델을 이용하여 얼굴 인식 프로젝트를 생성합니다.\n 디바이스 페이지에서 Deploy Project 버튼을 누릅니다.  프로젝트 페이지로 이동하면, Create new project 버튼을 누릅니다. Use a project template 을 선택하고, 템플릿 중, Face detection 을 선택하고\u0008 Next 버튼을 누릅니다. 설정을 건드리지 않고 Create 버튼을 누릅니다. 프로젝트가 생성되었는지 확인합니다. 만약 Description 등 항목이 나타나지 않을 경우, 페이지를 새로고침 해봅니다.  프로젝트를 기기에 배포하기 생성한 얼굴 인식 프로젝트를 DeepLens 기기에 배포합니다.\n 생성된 프로젝트를 클릭합니다. 우측 상단의 Deploy to device 버튼을 누릅니다. Target device 에 등록한 디바이스가 나타납니다. 선택 후 Review 버튼을 누릅니다. Deploy 버튼을 누릅니다. Deploy 가 완료될 때까지 기다립니다. 완료되면 Device status 가 online 으로 바뀝니다.   "},{"uri":"/deeplens-rekognition-workshop/lab2/","title":"프로젝트 출력 보기","tags":[],"description":"","content":"결과 화면 관찰하기 얼굴 인식 프로젝트에서 출력되는 결과 화면을 관찰합니다.\n이 워크샵에서는 브라우저를 이용해서 결과 화면을 관찰합니다.\n프로젝트 출력을 보는 다른 방법은 다음 링크에서 확인할 수 있습니다. 프로젝트 출력 보기\n브라우저에서 결과 화면 확인하기   DeepLens 에서 송출되는 화면을 받기 위해서, 인증서를 가져와야 합니다. 인증서를 가져와서 브라우저에 등록하는 방법은 다음 링크 에서 확인할 수 있습니다.\n  디바이스 화면에서 Video Streaming 항목에 있는 View Video stream 버튼을 누릅니다.   위 버튼은 당신을 https://{디바이스-ip-주소}:4000 로 데려갈 것입니다. 원하는 브라우저에 주소를 복사해서, 비디오 스트림을 관찰합니다.\n   "},{"uri":"/deeplens-rekognition-workshop/lab3/","title":"Amazon Rekognition 연동하기","tags":[],"description":"","content":"프로젝트 함수 변경하기  Lambda 콘솔 페이지 로 이동합니다. 함수 중에 deeplens-face-detection 이라는 함수를 클릭합니다. 이 함수가 DeepLens 기기에 설치되는 함수입니다. 함수의 코드를 모두 지우고, 다음 코드로 변경합니다.  from threading import Thread, Event\rimport os\rimport json\rimport numpy as np\rimport awscam\rimport cv2\rimport greengrasssdk\rimport sys\rimport urllib\rimport zipfile\rimport time\r# Latest software has boto3 installed\rtry:\rimport boto3\rexcept Exception:\rboto_dir = '/tmp/boto_dir'\rif not os.path.exists(boto_dir):\ros.mkdir(boto_dir)\rurllib.urlretrieve(\u0026quot;https://s3.amazonaws.com/dear-demo/boto_3_dist.zip\u0026quot;, \u0026quot;/tmp/boto_3_dist.zip\u0026quot;)\rwith zipfile.ZipFile(\u0026quot;/tmp/boto_3_dist.zip\u0026quot;, \u0026quot;r\u0026quot;) as zip_ref:\rzip_ref.extractall(boto_dir)\rsys.path.append(boto_dir)\rimport boto3\rdef mark_face(image, bbox, label=\u0026quot;\u0026quot;):\rH, W, _ = image.shape\rx = int(bbox[\u0026quot;Left\u0026quot;] * W)\ry = int(bbox[\u0026quot;Top\u0026quot;] * H)\rw = int(bbox[\u0026quot;Width\u0026quot;] * W)\rh = int(bbox[\u0026quot;Height\u0026quot;] * H)\rcv2.rectangle(image, (x, y), (x + w, y + h), (255, 165, 20), 10)\rtext_offset = 15\rcv2.putText(image, label,\r(x, y - text_offset),\rcv2.FONT_HERSHEY_SIMPLEX, 2.5, (255, 165, 20), 3) class LocalDisplay(Thread):\r\u0026quot;\u0026quot;\u0026quot; Class for facilitating the local display of inference results\r(as images). The class is designed to run on its own thread. In\rparticular the class dumps the inference results into a FIFO\rlocated in the tmp directory (which lambda has access to). The\rresults can be rendered using mplayer by typing:\rmplayer -demuxer lavf -lavfdopts format=mjpeg:probesize=32 /tmp/results.mjpeg\r\u0026quot;\u0026quot;\u0026quot;\rdef __init__(self, resolution):\r\u0026quot;\u0026quot;\u0026quot; resolution - Desired resolution of the project stream \u0026quot;\u0026quot;\u0026quot;\r# Initialize the base class, so that the object can run on its own\r# thread.\rsuper(LocalDisplay, self).__init__()\r# List of valid resolutions\rRESOLUTION = {'1080p' : (1920, 1080), '720p' : (1280, 720), '480p' : (858, 480)}\rif resolution not in RESOLUTION:\rraise Exception(\u0026quot;Invalid resolution\u0026quot;)\rself.resolution = RESOLUTION[resolution]\r# Initialize the default image to be a white canvas. Clients\r# will update the image when ready.\rself.frame = cv2.imencode('.jpg', 255*np.ones([640, 480, 3]))[1]\rself.stop_request = Event()\rdef run(self):\r\u0026quot;\u0026quot;\u0026quot; Overridden method that continually dumps images to the desired\rFIFO file.\r\u0026quot;\u0026quot;\u0026quot;\r# Path to the FIFO file. The lambda only has permissions to the tmp\r# directory. Pointing to a FIFO file in another directory\r# will cause the lambda to crash.\rresult_path = '/tmp/results.mjpeg'\r# Create the FIFO file if it doesn't exist.\rif not os.path.exists(result_path):\ros.mkfifo(result_path)\r# This call will block until a consumer is available\rwith open(result_path, 'w') as fifo_file:\rwhile not self.stop_request.isSet():\rtry:\r# Write the data to the FIFO file. This call will block\r# meaning the code will come to a halt here until a consumer\r# is available.\rfifo_file.write(self.frame.tobytes())\rexcept IOError:\rcontinue\rdef set_frame_data(self, frame):\r\u0026quot;\u0026quot;\u0026quot; Method updates the image data. This currently encodes the\rnumpy array to jpg but can be modified to support other encodings.\rframe - Numpy array containing the image data tof the next frame\rin the project stream.\r\u0026quot;\u0026quot;\u0026quot;\rret, jpeg = cv2.imencode('.jpg', cv2.resize(frame, self.resolution))\rif not ret:\rraise Exception('Failed to set frame data')\rself.frame = jpeg\rdef join(self):\rself.stop_request.set()\rdef infinite_infer_run():\r\u0026quot;\u0026quot;\u0026quot; Entry point of the lambda function\u0026quot;\u0026quot;\u0026quot;\rtry:\r# This face detection model is implemented as single shot detector (ssd).\rmodel_type = 'ssd'\routput_map = {1: 'face'}\r# Create an IoT client for sending to messages to the cloud.\rclient = greengrasssdk.client('iot-data')\riot_topic = '$aws/things/{}/infer'.format(os.environ['AWS_IOT_THING_NAME'])\r# Create a local display instance that will dump the image bytes to a FIFO\r# file that the image can be rendered locally.\rlocal_display = LocalDisplay('480p')\rlocal_display.start()\r# The sample projects come with optimized artifacts, hence only the artifact\r# path is required.\rmodel_path = '/opt/awscam/artifacts/mxnet_deploy_ssd_FP16_FUSED.xml'\r# Load the model onto the GPU.\rclient.publish(topic=iot_topic, payload='Loading face detection model')\rmodel = awscam.Model(model_path, {'GPU': 1})\rclient.publish(topic=iot_topic, payload='Face detection model loaded')\rrekog = boto3.client(\u0026quot;rekognition\u0026quot;)\rclient.publish(topic=iot_topic, payload='Rekognition client crearted')\r# Set the threshold for detection\rdetection_threshold = 0.25\r# The height and width of the training set images\rinput_height = 300\rinput_width = 300\r# Do inference until the lambda is killed.\rwhile True:\r# Get a frame from the video stream\rret, frame = awscam.getLastFrame()\rif not ret:\rraise Exception('Failed to get frame from the stream')\r# Resize frame to the same size as the training set.\rframe_resize = cv2.resize(frame, (input_height, input_width))\r# Run the images through the inference engine and parse the results using\r# the parser API, note it is possible to get the output of doInference\r# and do the parsing manually, but since it is a ssd model,\r# a simple API is provided.\rparsed_inference_results = model.parseResult(model_type,\rmodel.doInference(frame_resize))\r# Dictionary to be filled with labels and probabilities for MQTT\rcloud_output = {}\r# Get the detected faces and probabilities\rfor obj in parsed_inference_results[model_type]:\rif obj['prob'] \u0026gt; detection_threshold:\r# Store label and probability to send to cloud\rcloud_output[output_map[obj['label']]] = obj['prob']\r# If there is any face detected\rif cloud_output:\r# Make a rekognition api call\rret, jpeg = cv2.imencode(\u0026quot;.jpg\u0026quot;, frame)\rresp = rekog.recognize_celebrities(\rImage={\r\u0026quot;Bytes\u0026quot;: jpeg.tobytes()\r}\r)\r# Uncomment below to check result\r# client.publish(topic=iot_topic, payload=json.dumps(resp))\rcelebs = resp['CelebrityFaces']\r# Mark faces on the frame\rfor face in celebs:\rbbox = face[\u0026quot;Face\u0026quot;]['BoundingBox']\rmark_face(frame, bbox, label=face[\u0026quot;Name\u0026quot;])\runcelebs = resp[\u0026quot;UnrecognizedFaces\u0026quot;]\rfor face in uncelebs:\rbbox = face['BoundingBox']\rmark_face(frame, bbox, label=\u0026quot;\u0026quot;)\r# Set the next frame in the local display stream.\rlocal_display.set_frame_data(frame)\r# Send results to the cloud\rclient.publish(topic=iot_topic, payload=json.dumps(cloud_output))\rexcept Exception as ex:\rclient.publish(topic=iot_topic, payload='Error in face detection lambda: {}'.format(ex))\rinfinite_infer_run()\r기존 코드에서 바뀐 부분만 간단히 살펴보겠습니다.\ntry:\rimport boto3\rexcept Exception:\rboto_dir = '/tmp/boto_dir'\rif not os.path.exists(boto_dir):\ros.mkdir(boto_dir)\rurllib.urlretrieve(\u0026quot;https://s3.amazonaws.com/dear-demo/boto_3_dist.zip\u0026quot;, \u0026quot;/tmp/boto_3_dist.zip\u0026quot;)\rwith zipfile.ZipFile(\u0026quot;/tmp/boto_3_dist.zip\u0026quot;, \u0026quot;r\u0026quot;) as zip_ref:\rzip_ref.extractall(boto_dir)\rsys.path.append(boto_dir)\rimport boto3\rAWS Python SDK 인 boto3 를 임포트 하는 부분입니다. 최신 버전의 DeepLens 소프트웨어는 boto3 가 설치되어 있지만, 그렇지 않은 경우, boto3 패키지를 설치하고 임포트합니다.\ndef mark_face(image, bbox, label=\u0026quot;\u0026quot;):\rH, W, _ = image.shape\rx = int(bbox[\u0026quot;Left\u0026quot;] * W)\ry = int(bbox[\u0026quot;Top\u0026quot;] * H)\rw = int(bbox[\u0026quot;Width\u0026quot;] * W)\rh = int(bbox[\u0026quot;Height\u0026quot;] * H)\rcv2.rectangle(image, (x, y), (x + w, y + h), (255, 165, 20), 10)\rtext_offset = 15\rcv2.putText(image, label,\r(x, y - text_offset),\rcv2.FONT_HERSHEY_SIMPLEX, 2.5, (255, 165, 20), 3) cv2 를 이용해서, 이미지에 bounding box 와 라벨을 삽입하는 helper function 입니다.\n# If there is any face detected\rif cloud_output:\r# Make a rekognition api call\rret, jpeg = cv2.imencode(\u0026quot;.jpg\u0026quot;, frame)\rresp = rekog.recognize_celebrities(\rImage={\r\u0026quot;Bytes\u0026quot;: jpeg.tobytes()\r}\r)\r# Uncomment below to check result\r# client.publish(topic=iot_topic, payload=json.dumps(resp))\rcelebs = resp['CelebrityFaces']\r# Mark faces on the frame\rfor face in celebs:\rbbox = face[\u0026quot;Face\u0026quot;]['BoundingBox']\rmark_face(frame, bbox, label=face[\u0026quot;Name\u0026quot;])\runcelebs = resp[\u0026quot;UnrecognizedFaces\u0026quot;]\rfor face in uncelebs:\rbbox = face['BoundingBox']\rmark_face(frame, bbox, label=\u0026quot;\u0026quot;)\r만약 이미지에서 얼굴이 한 개라도 검출된다면 Rekognition API 를 호출하여 결과를 받아온 후, 유명인들의 얼굴에는 알맞은 라벨을 삽입하고, 결과가 없는 얼굴은 라벨이 없이 표시만 합니다.\n함수를 프로젝트에 적용하기  코드를 변경했으면, Deploy 를 눌러서 Lambda 함수를 저장합니다. 페이지 상단의 Action 을 눌러서 Publish new version 을 선택합니다.  DeepLens 프로젝트 콘솔 로 이동합니다. 우리가 생성한 얼굴 인식 프로젝트를 클릭합니다. Edit 버튼을 눌러 편집 화면으로 진입합니다. Project Content 부분에서 Function 을 누르면, 프로젝트에서 사용하고 있는 함수에 대한 정보를 변경할 수 있습니다. Version 을 누르면, 새로운 버전이 더 생성된 것을 확인할 수 있습니다. 새로운 버전을 선택합니다. Save 버튼을 눌러 저장합니다.  함수를 다시 디바이스에 배포하기 함수 배포하기 를 참조하여, 다시 함수를 디바이스에 배포합니다.\n결과 확인하기 프로젝트 출력 보기 에서 한 것처럼, View video stream 주소로 가서, 결과를 확인합니다.\n"},{"uri":"/deeplens-rekognition-workshop/lab3/lab3.1/","title":"여러 개의 얼굴 인식하기 (심화)","tags":[],"description":"","content":"인식하려는 얼굴을 위한 S3 생성  S3 버켓을 하나 생성합니다. 인식하려는 얼굴 사진을 업로드 합니다.  DeepLens 람다 함수 템플릿 구성 AWS 딥렌즈 람다 함수 템플릿 을 다운 받은 후 압축해제를 합니다.\n폴더 내에는 다음과 같은 파일들이 들어 있습니다.\n lambda_function.py: 딥렌즈 내부에서 돌아가는 실제 함수입니다. local_display.py: 결과 영상 송출을 위한 파일입니다. greengrasssdk: AWS Greengrass 배포를 위한 sdk 입니다.  여러 개의 얼굴을 인식하기 위해서 다음 코드 를 이용할 것입니다. lambda_function.py 를 열고 내용을 모두 지운 후, 코드를 붙여넣기 합니다.\n 60번째 줄의 {s3-bucket-name} 을 얼굴 사진이 들어 있는 버켓 이름으로 변경합니다.  def get_resp(client, img_key, img_bytes):\rtry:\rreturn {\r\u0026quot;key\u0026quot;: img_key,\r\u0026quot;resp\u0026quot;: client.compare_faces(\rSourceImage={\r'S3Object': {\r'Bucket': '{s3 bucket name}',\r'Name': img_key\r}\r},\rTargetImage={\r'Bytes': img_bytes\r}\r)\r}\rexcept Exception:\rreturn None\r104번째 줄의 target_dict 를 업데이트합니다. key는 s3 오븐젝트 키를, value 는 각 사진에 해당하는 label 을 작성합니다.  target_dict = {\r\u0026quot;list.jpeg\u0026quot;: \u0026quot;Names\u0026quot;,\r\u0026quot;of.png\u0026quot;: \u0026quot;of\u0026quot;,\r\u0026quot;s3_keys.jpeg\u0026quot;: \u0026quot;faces\u0026quot;\r}\r전체 폴더를 다시 zip 파일로 압축합니다.  람다 함수 생성하기  AWS Lambda 콘솔 에 들어갑니다. \u0026ldquo;함수 생성\u0026quot;을 클릭합니다. 함수 이름을 작성합니다. 이때 함수 이름은 deeplens 로 시작해야 합니다. 예) deeplens_custom_face 런타임은 Python 3.7 을 선택합니다. 함수를 생성합니다. 코드 소스 부분으로 이동한 후, \u0026ldquo;에서 업로드 -\u0026gt; .zip 파일\u0026rdquo; 을 클릭합니다. 위에서 만든 zip 파일을 업로드합니다. 페이지 위로 올라가서 \u0026ldquo;작업 -\u0026gt; 새 버전 발행\u0026rdquo; 을 클릭합니다. 새로운 버전을 발행합니다.  딥렌즈 프로젝트 생성  AWS DeepLens 콘솔 에 들어갑니다. Projects 를 클릭합니다. Create new project 클릭하고, Create a new blank project 를 클릭합니다. 프로젝트 이름과 설명을 작성합니다. Project content 에서 Add model 을 한 후, deeplens-face-detection 을 선택합니다. 앞의 과정에서 face-detection 샘플 프로젝트를 배포했다면, 모델이 있을 것입니다. Project content 에서 Add function 을 한 후, 위에서 배포한 람다 함수를 선택합니다. 디바이스로 배포합니다.  "},{"uri":"/deeplens-rekognition-workshop/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"/deeplens-rekognition-workshop/tags/","title":"Tags","tags":[],"description":"","content":""},{"uri":"/deeplens-rekognition-workshop/credits/","title":"크레딧","tags":[],"description":"","content":"패키지와 라이브러리  mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support  도구  Netlify - Continuous deployement and hosting of this documentation Hugo  "}]